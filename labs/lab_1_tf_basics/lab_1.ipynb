{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: TensorFlow Basics (2.67/3)\n",
    "\n",
    "## S1: 2/3, S2: 2.5/3, S3: 3/3, (2.5 + 2.5 + 3)/3 = 2.67"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Tensors (2.5/3)\n",
    "\n",
    "If you want to review any of this material, look over https://docs.scipy.org/doc/numpy-1.15.0/user/quickstart.html and https://www.tensorflow.org/guide/tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1: Tensor values, shapes, rank, and axes\n",
    "Make tensor values by hand (e.g. `x = np.array([[1, 2, 3], [4, 5, 6]])`) of the following shapes:\n",
    " * a: (2, 2)\n",
    " * b: (3)\n",
    " * c: (3, 1)\n",
    " * d: (1, 3)\n",
    " * e: ()\n",
    " * f: (1)\n",
    " * g: (2, 2, 2)\n",
    " * h: (2, 3, 1, 2)\n",
    " \n",
    " For each, put its tensor rank and total number of elements in a comment.\n",
    " Yes, this is pretty boring, but it's also short and it's really important to understand what tensors of different shapes look like and how shapes, rank, and axes interact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank 2\n",
    "# 4 elements\n",
    "a = tf.constant([[1, 2], [3, 4]])\n",
    "\n",
    "# Rank 1\n",
    "# 3 elements\n",
    "b = tf.constant([1, 2, 3])\n",
    "\n",
    "# Rank 2\n",
    "# 3 elements\n",
    "c = tf.constant([[1], [2], [3]])\n",
    "\n",
    "# Rank 2\n",
    "# 3 elements\n",
    "d = tf.constant([[1, 2, 3]])\n",
    "\n",
    "# Rank 0\n",
    "# 1 element\n",
    "e = tf.constant(1)\n",
    "\n",
    "# Rank 1\n",
    "# 1 element\n",
    "f = tf.constant([1])\n",
    "\n",
    "# Rank 3\n",
    "# 8 elements\n",
    "g = tf.constant([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
    "\n",
    "# Rank 4\n",
    "# 12 elements\n",
    "h = tf.constant([[[[1, 2]], [[3, 4]], [[5, 6]]], [[[7, 8]], [[9, 10]], [[11, 12]]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Slices and reductions\n",
    "Use slicing or `tf.reduce_mean`, `tf.reduce_sum`, and `tf.reduce_any` on the tensors defined below to print:\n",
    " * The (1-2-3)-st element of `a`\n",
    " * The first column of `b`\n",
    " * The shape-(2, 3, 2) tensor obtained by selecting the second and third elements of the third axis of `a`\n",
    " * The sum of all values in `b`\n",
    " * The 2-vector containing means of each row of `b` \n",
    " * The (1, 3) tensor containing, for each column in `c`, whether that column contains any `True` values\n",
    " \n",
    "Each statement should take the form \n",
    "```\n",
    "tf.print(something[...])\n",
    "```\n",
    "or \n",
    "```\n",
    "tf.print(tf.reduce_something(...))\n",
    "```\n",
    "Follow each with a comment stating the shape of the output.\n",
    "For a rank-2 tensor, the first index specifies row and the second specifies column.\n",
    "Make sure to pay attention to the `axis` and `keepdims` arguments of the `reduce` functions. Also, recall that TensorFlow is **0-indexed**.\n",
    " \n",
    " \n",
    "For this problem, I'll set up the name scope, but for all future problems you'll need to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(np.ones((2, 3, 4))) # Tensor of ones with shape (2, 3, 4)\n",
    "b = tf.constant([[1., 2.], \n",
    "                 [3., 4.]]) # Tensor of the matrix [1 2; 3 4] with shape (2, 2)\n",
    "c = tf.constant([[True, True, False],\n",
    "                 [False, True, False]]) # Binary tensor with shape (2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[1 3]\n",
      "[[[1 1]\n",
      "  [1 1]\n",
      "  [1 1]]\n",
      "\n",
      " [[1 1]\n",
      "  [1 1]\n",
      "  [1 1]]]\n",
      "10\n",
      "[1.5 3.5]\n",
      "[1 1 0]\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('slices_and_reductions'):\n",
    "    # Your code here\n",
    "    tf.print(a[0, 1, 2])\n",
    "    # Shape: scalar\n",
    "\n",
    "    tf.print(b[:, 0])\n",
    "    # Shape: (2)\n",
    "\n",
    "    tf.print(a[:, :, 1:3])\n",
    "    # Shape: (2, 3, 2)\n",
    "\n",
    "    tf.print(tf.reduce_sum(b))\n",
    "    # Shape: scalar\n",
    "\n",
    "    tf.print(tf.reduce_mean(b, 1))\n",
    "    # Shape: (2)\n",
    "    \n",
    "#   TODO: (-0.5) the instructions asked for a tensor of shape (1, 3), thus you would need to specify \"keepdims = True\" in the arguments of reduce_any. \n",
    "    tf.print(tf.reduce_any(c, 0))\n",
    "    # Shape: (3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3: Transposition and reshaping\n",
    "Use `tf.transpose` to print:\n",
    " * `b` with its rows and columns swapped\n",
    " * `a` with its second and third axes swapped; comment its shape\n",
    " \n",
    "Use `tf.reshape` to print:\n",
    " * The values of `b` in a tensor with shape (1, 4)\n",
    " * The values of `b` in a tensor with shape (4, 1)\n",
    " \n",
    "Do this all inside the name scope \"transposition_and_reshaping\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 3]\n",
      " [2 4]]\n",
      "[[[1 1 1]\n",
      "  [1 1 1]\n",
      "  [1 1 1]\n",
      "  [1 1 1]]\n",
      "\n",
      " [[1 1 1]\n",
      "  [1 1 1]\n",
      "  [1 1 1]\n",
      "  [1 1 1]]]\n",
      "[[1 2 3 4]]\n",
      "[[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "with tf.name_scope('transposition_and_reshaping'):\n",
    "    tf.print(tf.transpose(b))\n",
    "    tf.print(tf.transpose(a, perm=[0, 2, 1]))\n",
    "    # Shape: (2, 4, 3)\n",
    "    \n",
    "    tf.print(tf.reshape(b, [1, 4]))\n",
    "    tf.print(tf.reshape(b, [4, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Computing with Operations and Graphs (2.5/3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1: The dot product (as a sum of scalar products)\n",
    "Write a function `dot_sum()` that takes in two rank-1 tensors `a` and `b` of equal shape and returns a tensor that holds their dot product, $$\\text{result} = a \\cdot b = \\sum_{i = 1}^{\\dim{a}} a_i \\cdot b_i $$\n",
    "\n",
    "The computation should first multiply the elements in $a$ and $b$ into a vector $a \\odot b$ (the [Hadamard product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)) of $a$ and $b$), then sum across the vector to produce a scalar. \n",
    "Your implementation should be _vectorized_: it should not explicitly use the shape of an input tensor or do any looping.\n",
    "The tensor output by your function must be rank-0.\n",
    "\n",
    "The entire computation should use the name scope \"dot_sum\" and the tensor you return should have the name \"result\".\n",
    "\n",
    "TensorFlow operations to look at:\n",
    " * `tf.multiply` (or equivalently, the binary operation *)\n",
    " * `tf.reduce_sum`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_sum(a, b):\n",
    "    '''\n",
    "    Given rank-1 tensors a and b with equal shapes, return the dot product \n",
    "    of a and b as a rank-0 tensor computed via Hadamard product.\n",
    "    '''\n",
    "    # Your code here\n",
    "    with tf.name_scope('dot_sum'):\n",
    "        prod = tf.multiply(a, b)\n",
    "#         TODO: (-0.25) the instructions asked for result to have the name \"result\". Thus you would have to include the argument \"name = 'result'\" when calling the reduce_sum method \n",
    "        result = tf.reduce_sum(prod)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([1, 2, 3])\n",
    "b = tf.constant([4, 5, 6])\n",
    "tf.print(dot_sum(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2: The dot product (as matrix multiplication)\n",
    "Write a function `dot_multiply()` that takes in two rank-1 tensors `a` and `b` of equal shape and returns a tensor that holds their dot product, $$\\text{result} = a \\cdot b = a^T b $$\n",
    "\n",
    "The computation should use `tf.matmul` to perform the multiplication, which expects that your input tensors have rank of at least two (they should be matrices, not vectors).\n",
    "Since your input vectors are rank-1, this means you'll need to use `tf.expand_dims` with `axis=-1` to add a \"dummy dimension\".\n",
    "This is a subtle but important point: your vectors start with shape [n], but matrix multiplication is only defined for matrices with shapes [1, n] and [n, 1].\n",
    "Depending on how you do it, you will probably get a rank-2 tensor with a shape like [1, 1].\n",
    "You must return a rank-0 tensor, so use `tf.squeeze` to eliminate dummy dimensions.\n",
    "\n",
    "The entire computation should use the name scope \"dot_multiply\" and the tensor you return should have the name \"result\".\n",
    "This will not collide with the previous \"result\" tensor because of name scoping.\n",
    "(If it did, it would be renamed to \"result_0\" in the graph)\n",
    "\n",
    "TensorFlow operations to look at:\n",
    " * `tf.matmul`\n",
    " * `tf.transpose`\n",
    " * `tf.expand_dims`\n",
    " * `tf.squeeze`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expand_dims_v2() got an unexpected keyword argument 'in_place'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-e5b26e6d8251>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_place\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expand_dims_v2() got an unexpected keyword argument 'in_place'"
     ]
    }
   ],
   "source": [
    "a = tf.constant([1, 2, 3])\n",
    "tf.print(a)\n",
    "tf.expand_dims(a, 0, in_place=True)\n",
    "tf.print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "tf.Tensor([[1 2 3]], shape=(1, 3), dtype=int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-23 18:36:16.910762: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-04-23 18:36:16.911212: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=32>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dot_multiply(a, b):\n",
    "    '''\n",
    "    Given rank-1 tensors a and b with equal shapes, return the dot product \n",
    "    of a and b as a rank-0 tensor computed via matrix multiplication.\n",
    "    '''\n",
    "    # Your code here\n",
    "    with tf.name_scope('dot_multiply'):\n",
    "        a_mat = tf.expand_dims(a, 0)\n",
    "        a_mat_trans = tf.transpose(a_mat)\n",
    "        b_mat = tf.expand_dims(b, 0)\n",
    "        prod = tf.matmul(b_mat, a_mat_trans)\n",
    "        result = tf.squeeze(prod)\n",
    "        return result\n",
    "        \n",
    "\n",
    "a = tf.constant([1, 2, 3])\n",
    "b = tf.constant([4, 5, 6])\n",
    "dot_multiply(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3: A single ReLU unit\n",
    "The \"default\" activation function for modern neural networks is the [rectified linear unit](https://en.wikipedia.org/wiki/Rectifier_(neural_networks) (or \"ReLU\"):\n",
    "$$ \\text{relu}(x) = max(0, x). $$\n",
    "\n",
    "In a neural network using ReLU activation, a single unit with $n$ inputs has parameters $w$ (an $n$-vector of weights) and $b$ (a scalar).\n",
    "It computes the function\n",
    "$$ f(x; w, b) = \\text{relu}(w \\cdot x + b). $$\n",
    "\n",
    "Using either `dot_sum` or `dot_multiply`, add these tensors and operations to the default graph:\n",
    "$$\n",
    "\\begin{align}\n",
    "&x: \\space \\text{placeholder} \\\\\n",
    "&w = \\begin{bmatrix}2 & 0.5 & -1\\end{bmatrix} \\\\\n",
    "&b = 0.3 \\\\\n",
    "&\\text{state} = w \\cdot x + b \\\\\n",
    "&\\text{activation} = \\max(\\text{state}, 0)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "`x` should have shape [3] and dtype `tf.float32`, and all tensors should be named, under the name scope \"ReLU\".\n",
    "This includes the tensors created through your dot product function, but do not change your implementation to add to the name! Then wrap all of this in a function called `relu` that takes in one argument to initialize the `tf.Variable` `x` object, then returns `activation`.\n",
    "\n",
    "Then, print `relu` for:\n",
    " * $x = \\begin{bmatrix} 1 & 1 & 1 \\end{bmatrix}$\n",
    " * $x = \\begin{bmatrix} -1 & 2 & 0 \\end{bmatrix}$\n",
    " * $x = \\begin{bmatrix} 1 & 0 & 0 \\end{bmatrix}$\n",
    " * $x = \\begin{bmatrix} 0 & 0 & 0 \\end{bmatrix}$\n",
    "\n",
    "TensorFlow operations to look at:\n",
    " * tf.constant\n",
    " * tf.Variable\n",
    " * tf.add\n",
    " * tf.maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your graph code here\n",
    "def relu(x):\n",
    "    with tf.name_scope('ReLU'):\n",
    "#         TODO: (-0.25) b should be specified using tf.constant and w, b, state, and activation should have a name argument. This might seem useless now, but correctly setting up the comptuational graph can help with visualization and debugging for complicated architectures. \n",
    "        w = tf.constant([2, 0.5, -1])\n",
    "        b = 0.3\n",
    "        state = tf.add(dot_multiply(w, tf.cast(x, float)), b)\n",
    "        activation = tf.maximum(state, 0)\n",
    "        return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8\n",
      "0\n",
      "2.3\n",
      "0.3\n"
     ]
    }
   ],
   "source": [
    "# Your print code here\n",
    "tf.print(relu(tf.constant([1, 1, 1])))\n",
    "tf.print(relu(tf.constant([-1, 2, 0])))\n",
    "tf.print(relu(tf.constant([1, 0, 0])))\n",
    "tf.print(relu(tf.constant([0, 0, 0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aside on activation functions\n",
    "\n",
    "One way to derive feedforward neural networks is to begin by saying \"I'd like to do a simple (linear) transformation on my input features to make them easier to model, then use a simple model (e.g. linear regression) that instead uses the transformed features.\"\n",
    "Doing this means your total model is $y = ABx + b$ where $B$ is the matrix multiplying an input point $x$ into a new representation and $A$ is the matrix parameterizing the linear regression.\n",
    "\n",
    "But, $AB$ is just another matrix, and so by adding a representation you have not made your model more powerful; instead if you'd \"twisted\" the input space after appyling B, the overall map would be nonlinear and the composite model would have greater representation power.\n",
    "Activation functions perform this \"twisting\".\n",
    "Deep neural networks come from the observation that it'd be easier to get a good representation (top layer) if it was based on a lower-level representation (early layers).\n",
    "\n",
    "Here's a great article explaining the geometric interpretation of activation functions: https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/.\n",
    "The general idea is that neural networks can learn parameters that use the \"twists\" such that the entire network deforms space so that the manifold defined by your input data is simple. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Optimization (3/3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizing a function with gradient descent\n",
    "Minimize the scalar function $f(x) = (x-1)(2x-2)(x-3)(x-4)$, plotted below, using gradient descent.\n",
    "It has a local minimum near $x = 1$ and a global minimum near $x = 3.5$.\n",
    "\n",
    "![f(x)](./images/plot_f.png)\n",
    "\n",
    "The steps to build the graph are:\n",
    " 1. Use `tf.Variable` to create a variable named `x` that uses a `np.random.uniform` on the range [-1, 5] to initialize.\n",
    " 2. Make a `tf.optimizers.SGD` named \"optimizer\" with a learning rate of 0.01.\n",
    " 3. Make a function `optimize` that takes in an optimizer as an argument and represents each step of the training loop.\n",
    " 4. Create a `tf.GradientTape` object and make a tensor `y` that represents $f(x)$ given a value of `x` under it.\n",
    " 5. Get the gradients of `y` from the `tf.GradientTape` and apply them to the optimizer.\n",
    " \n",
    "Remember steps 4 and 5 go inside the `optimize` function!\n",
    "The whole subgraph for this problem should go under a name scope of \"minimize_f\", and operations to compute `y` should have an additional name scope of \"compute_f\". \n",
    "\n",
    "In a comment, rewrite the `optimize` function using the `minimize` function on the optimizer instead of getting the gradients and applying them. Is `tf.GradientTape` is necessary? You do not need to worry about `tf.name_scope` for this.\n",
    "\n",
    "Then, the steps to minimize the function once are:\n",
    " 1. Print the initial values of `x` and `y`.\n",
    " 2. Run `minimize` 1000 times.\n",
    " 3. Print the final values of `x` and `y`.\n",
    " \n",
    "Minimize the function a few times. If you did it right, you'll find that in each run the optimizer finds one of two minima. Running minimization a few times, you should see it find both eventually. What determines which minimum is found? Answer in the markdown box below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your graph code here\n",
    "def optimize(opt):\n",
    "    with tf.name_scope('minimize_f'):\n",
    "        with tf.GradientTape() as g:\n",
    "            g.watch(x)\n",
    "            with tf.name_scope('compute_f'):\n",
    "                y = 2 * (x - 1)**2 * (x - 3) * (x - 4)\n",
    "        dy_dx = g.gradient(y, x)\n",
    "        opt.apply_gradients(zip([dy_dx], [x]))\n",
    "        \n",
    "# Write the optimize function using the minimize method\n",
    "# def optimize(opt):\n",
    "#     y = 2 * (x - 1)**2 * (x - 3) * (x - 4)\n",
    "#     opt.minimize(zip([y], [x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.00000024\n",
      "1.00000024\n",
      "3.59307051\n",
      "3.59307051\n",
      "1.00000024\n",
      "0.999999881\n",
      "0.999999881\n",
      "3.59307051\n",
      "0.999999881\n",
      "3.59307\n"
     ]
    }
   ],
   "source": [
    "# Your training loop here\n",
    "for i in range(10):\n",
    "    x = tf.Variable(np.random.uniform(-1, 5))\n",
    "    opt = tf.optimizers.SGD(learning_rate=0.01)\n",
    "    for i in range(1000):\n",
    "        optimize(opt)\n",
    "    tf.print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random value of x determines which minimum will be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
